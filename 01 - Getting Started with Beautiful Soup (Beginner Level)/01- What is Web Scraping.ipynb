{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755947ed",
   "metadata": {},
   "source": [
    "# ğŸ“ Lesson 1: What is Web Scraping?\n",
    "ğŸ“˜ Introduction\n",
    "\n",
    "Web Scraping is the automated process of extracting information from websites. Instead of manually copying data, we write Python code to gather data quickly and accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c80963",
   "metadata": {},
   "source": [
    "## Why Learn Web Scraping?\n",
    "- ğŸ” Automate data collection (prices, news, stocks, weather)\n",
    "- ğŸ’¼ Analyze job markets, product catalogs, or competitor sites\n",
    "- ğŸ“Š Collect data for machine learning or data science projects\n",
    "- ğŸ“„ Archive or monitor changes on public pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef43fa",
   "metadata": {},
   "source": [
    "## ğŸ” Is Web Scraping Legal?\n",
    "Web scraping is legal when done responsibly. You should always:\n",
    "- âœ… Check the siteâ€™s robots.txt\n",
    "- âœ… Respect rate limits and avoid overloading servers\n",
    "- âŒ Never scrape private, personal, or sensitive data\n",
    "- âŒ Avoid login-based scraping unless on test/demo sites\n",
    "\n",
    "ğŸ’¡ Remember: Just because data is publicly viewable doesnâ€™t always mean itâ€™s legally usable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8612bb",
   "metadata": {},
   "source": [
    "### ğŸ“œ robots.txt: What Is It?\n",
    "Most websites include a file called robots.txt to tell bots what pages are allowed or disallowed for crawling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0fcdc6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```txt\n",
    "User-agent: *\n",
    "Disallow: /private/\n",
    "Allow: /public/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a654b",
   "metadata": {},
   "source": [
    "You can access it by visiting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fbb36",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "https://<domain>/robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84676732",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734af66b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "https://www.wikipedia.org/robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5244f5",
   "metadata": {},
   "source": [
    "## âœ… Test Sites for Ethical Practice\n",
    "Here are a few great websites built just for practicing scraping:\n",
    "\n",
    "| Site | Description |\n",
    "| -------- | ------- |\n",
    "| scrapethissite.com | Learn scraping, pagination, and login |\n",
    "| quotes.toscrape.com | Safe environment to practice login and session |\n",
    "| books.toscrape.com | Practice scraping books and prices |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fafdfd",
   "metadata": {},
   "source": [
    "## ğŸ“Œ Summary\n",
    "\n",
    "| Concept          | Description                                                  |\n",
    "| ---------------- | ------------------------------------------------------------ |\n",
    "| Web Scraping     | Extracting data from websites automatically                  |\n",
    "| robots.txt       | A guideline file that tells bots where theyâ€™re allowed       |\n",
    "| Ethical Scraping | Following rules, being polite, never scraping sensitive data |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4410467",
   "metadata": {},
   "source": [
    "## ğŸ’» No Code in This Lesson\n",
    "In the next lesson, weâ€™ll install the required tools and create our first BeautifulSoup object ğŸ¥£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b24b94",
   "metadata": {},
   "source": [
    "## âœ… Your Task\n",
    "1. Visit 2â€“3 websites and check their robots.txt file\n",
    "\n",
    "2. Explore https://scrapethissite.com for practice\n",
    "\n",
    "3. Write a short note for yourself: What do you want to scrape and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbbc367",
   "metadata": {},
   "source": [
    "## ğŸ§ª Ready for some hands-on work? \n",
    "Let's move to Lesson 2: Setting Up Your Environment where weâ€™ll install the necessary packages and set up a virtual environment."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

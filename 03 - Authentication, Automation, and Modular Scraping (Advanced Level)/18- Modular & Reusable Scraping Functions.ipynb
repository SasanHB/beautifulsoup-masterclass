{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39aa8424",
   "metadata": {},
   "source": [
    "# ğŸ“ Lesson 18: Modular & Reusable Scraping Functions\n",
    "\n",
    "ğŸ¯ Goal\n",
    "\n",
    "In this lesson, you'll learn how to:\n",
    "\n",
    "- Turn scraping code into **reusable Python functions**\n",
    "\n",
    "- Separate logic into clean modules (fetch, parse, save)\n",
    "\n",
    "- Make your scrapers more readable, maintainable, and testable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d5861",
   "metadata": {},
   "source": [
    "## âœ… Why Modular Code Matters\n",
    "\n",
    "| Benefit            | Example                                            |\n",
    "| ------------------ | -------------------------------------------------- |\n",
    "| ğŸ” Reusability     | Scrape any tag, URL, or category with one function |\n",
    "| ğŸ”§ Maintainability | Change one part without breaking the rest          |\n",
    "| ğŸ§ª Testability     | Easily test parts of the scraper individually      |\n",
    "| ğŸ” Readability     | Cleaner and easier to debug                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65374940",
   "metadata": {},
   "source": [
    "## âœ… Example: Scraping Quotes by Tag (Modularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b30e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "def get_soup(url, headers=None, delay=True):\n",
    "    \"\"\"Send a GET request and return parsed soup.\"\"\"\n",
    "    if delay:\n",
    "        sleep_time = round(random.uniform(1.5, 3.0), 2)\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    response = requests.get(url, headers=headers or get_default_headers())\n",
    "    return BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "\n",
    "def get_default_headers():\n",
    "    \"\"\"Return a browser-like headers dictionary.\"\"\"\n",
    "    return {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/113.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_quotes(soup):\n",
    "    \"\"\"Extract all quotes from a page soup.\"\"\"\n",
    "    quotes_data = []\n",
    "    quote_blocks = soup.select(\"div.quote\")\n",
    "\n",
    "    for quote in quote_blocks:\n",
    "        text = quote.select_one(\"span.text\").text.strip()\n",
    "        author = quote.select_one(\"small.author\").text.strip()\n",
    "        quotes_data.append({\"text\": text, \"author\": author})\n",
    "    \n",
    "    return quotes_data\n",
    "\n",
    "\n",
    "def scrape_quotes_by_tag(tag, pages=1):\n",
    "    \"\"\"Scrape quotes for a given tag and number of pages.\"\"\"\n",
    "    all_quotes = []\n",
    "    base_url = f\"https://quotes.toscrape.com/tag/{tag}/page/{{}}/\"\n",
    "\n",
    "    for page_num in range(1, pages + 1):\n",
    "        url = base_url.format(page_num)\n",
    "        print(f\"ğŸ” Scraping {url} ...\")\n",
    "        soup = get_soup(url)\n",
    "        quotes = extract_quotes(soup)\n",
    "\n",
    "        if not quotes:\n",
    "            print(\"âš ï¸ No more quotes found.\")\n",
    "            break\n",
    "        \n",
    "        all_quotes.extend(quotes)\n",
    "\n",
    "    return all_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4546be35",
   "metadata": {},
   "source": [
    "### âœ… Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f46183",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tag = input(\"Enter a quote tag to scrape: \").strip()\n",
    "    quotes = scrape_quotes_by_tag(tag, pages=3)\n",
    "\n",
    "    for q in quotes:\n",
    "        print(f\"ğŸ“ {q['text']} â€” {q['author']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d38a8b",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Bonus Tip: Use .env Files or Config Modules\n",
    "\n",
    "Keep base URLs, delays, and headers in a `config.py` or `.env` for easier project management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b80f32",
   "metadata": {},
   "source": [
    "## Practice Tasks\n",
    "\n",
    "1. Move each function to a separate file/module (e.g., `utils.py`, `scraper.py`)\n",
    "\n",
    "2. Add a `save_to_csv(data, filename)` function\n",
    "\n",
    "3. Make the tag scraper work with Selenium instead of requests\n",
    "\n",
    "4. Add error handling to `get_soup()` if the request fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6066910",
   "metadata": {},
   "source": [
    "## ğŸ”œ Next up: Lesson  19 â€“ Logging, Debugging, and Testing Your Scraper\n",
    "\n",
    "Learn how to add logs, handle exceptions smartly, and structure your code for testing."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

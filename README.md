# ğŸœ Beautiful Soup Masterclass

Welcome to the **Beautiful Soup Masterclass** a complete hands-on course that teaches you how to **scrape, parse, and extract data from websites** using Python ğŸ and the legendary **Beautiful Soup** library.

---

## ğŸ¯ What Youâ€™ll Learn

By the end of this course, you'll be able to:

- âœ¨ Learn how to extract data from websites  
- ğŸ” Parse and search HTML like a pro  
- âš™ Extract elements using tag names, classes, and CSS selectors  
- ğŸ§° Build reusable scrapers using Python functions  
- ğŸ’¾ Save your results in CSV, JSON, or databases  
- ğŸ›¡ Understand ethical scraping and anti-bot protection and avoid getting blocked  
- ğŸš€ Apply your skills in real-world mini-projects

---

## ğŸ“š Course Structure

This course is divided into **4 parts** with multiple lessons and projects:
Each part contains multiple Jupyter Notebooks with live examples, comments, and exercises ğŸ§ª. Youâ€™ll also find best practices, debugging tips, and extra resources.
### ğŸŸ© Part 1 â€“ Getting Started with Beautiful Soup (Beginner Level)

| #   | Title                                 |
|-----|---------------------------------------|
| 01  | ğŸ“– What is Web Scraping?              |
| 02  | ğŸ›  Setting Up Your Environment         |
| 03  | ğŸœ Creating a BeautifulSoup Object     |
| 04  | ğŸŒ³ Navigating the HTML Tree           |
| 05  | ğŸ¯ CSS Selectors with `.select()`     |
| 06  | ğŸ§¹ Cleaning Extracted Data            |
| 07  | âš  Error Handling Basics               |

### ğŸŸ¨ Part 2 â€“ Data Extraction and Web Navigation (Intermediate Level)

| #   | Title                                     |
|-----|-------------------------------------------|
| 08  | ğŸ“Š Extracting Tables and Lists            |
| 09  | ğŸ” Advanced Searching (Regex, Lambda)     |
| 10  | ğŸ“„ Pagination and Multi-page Scraping     |
| 11  | ğŸ§¾ Forms and Query URLs                   |
| 12  | â± Polite Scraping (Delay & Headers)      |
| 13  | ğŸ’¾ Saving to CSV, JSON, and SQLite        |

### ğŸŸ¥ Part 3 â€“ Authentication, Automation, and Modular Scraping (Advanced Level)

| #   | Title                                        |
|-----|----------------------------------------------|
| 14  | ğŸ” Using `requests.Session()` & Cookies      |
| 15  | ğŸ§¬ Regex in BeautifulSoup                    |
| 16  | ğŸ§  Scraping JavaScript Sites (Intro to Selenium) |
| 17  | ğŸ•µ Bypassing Anti-Bot Mechanisms             |
| 18  | ğŸ§© Modular & Reusable Scraping Functions     |
| 19  | ğŸ§ª Logging, Debugging, and Testing           |

### ğŸŸ¦ Part 4 â€“ Practical and Applied Scraping Projects

| #   | Project                                  |
|-----|------------------------------------------|
| 01  | ğŸ— News Headlines Extractor               |
| 02  | ğŸ›’ E-commerce Price Tracker               |
| 03  | ğŸ’¼ Job Listings Aggregator                |
| 04  | â˜ Weather Forecast Scraper               |
| 05  | â‚¿ Crypto Price Monitor (Advanced)        |
| 06  | ğŸ˜ Real Estate Listings Analyzer          |

---

## ğŸš€ Setup Instructions

### ğŸ“¦ Install Dependencies

Make sure you are in the root of the repo and then run:

```bash
pip install -r requirements.txt
```

**requirements.txt** includes:

- beautifulsoup4  
- requests  
- lxml  
- jupyter

You can also install them manually if needed:

```bash
pip install beautifulsoup4 requests lxml notebook
```

---

## ğŸ‘¨â€ğŸ« How to Use This Course

Each Jupyter Notebook:

- ğŸ“˜ Explains the topic in simple language  
- âœ… Contains commented code examples  
- ğŸ§ª Encourages you to experiment and learn by doing  
- ğŸ“ Uses real websites or datasets where possible  

ğŸ§‘â€ğŸ’» Open any `.ipynb` file in Jupyter Notebook or VS Code to get started!

---

## ğŸ’¬ Questions or Suggestions?

Feel free to open an issue or contact me via GitHub. Contributions are welcome! ğŸŒ

---

## ğŸ›‘ Disclaimer: Ethical Web Scraping Notice

This educational project is provided **for learning purposes only**.

All scraping examples:

- ğŸ” Use **publicly available** data  
- ğŸ“ Respect each websiteâ€™s **robots.txt** rules and legal boundaries
- â± Implement **delay mechanisms** to avoid overloading servers  
- ğŸ” Demonstrate session handling only on safe and **non-sensitive websites**
- ğŸš« Avoid accessing or bypassing **personal accounts, CAPTCHA systems**, or **private data**
- ğŸ“œ Are intended to **educate developers**, not to bypass website policies

Please always check a website's `robots.txt` file and Terms of Service before scraping.

---

> â— This project does not promote or support any form of unethical or unauthorized data extraction. Users are fully responsible for ensuring compliance with legal and ethical standards when applying these techniques.
> ğŸ’¡ Use this knowledge ethically and responsibly.

ğŸ“Œ Let's start scraping the web â€” ethically, responsibly, and like a pro! ğŸ’ªğŸŒ

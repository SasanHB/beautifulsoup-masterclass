{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870239ec",
   "metadata": {},
   "source": [
    "# ğŸ“ Lesson 10: Pagination and Multi-Page Scraping\n",
    "\n",
    "ğŸ¯ Goal\n",
    "\n",
    "In this lesson, you'll learn how to:\n",
    "\n",
    "1. Identify pagination links in HTML\n",
    "2. Scrape data from multiple pages\n",
    "3. Automate following â€œNextâ€ buttons\n",
    "4. Combine results across pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f73c1b",
   "metadata": {},
   "source": [
    "ğŸ’» Practice Site\n",
    "\n",
    "ğŸ“ https://quotes.toscrape.com/page/1/\n",
    "\n",
    "This site has classic pagination and is perfect for BeautifulSoup!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c41d1",
   "metadata": {},
   "source": [
    "## âœ… Step-by-Step Pagination Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b703931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ğŸ—‚ï¸ Base URL with a placeholder for page numbers\n",
    "base_url = \"https://quotes.toscrape.com/page/{}/\"\n",
    "\n",
    "# ğŸ” Loop through page numbers 1 to 5\n",
    "for page_num in range(1, 6):  # You can adjust the range as needed\n",
    "    print(f\"ğŸ“„ Scraping page {page_num}...\")\n",
    "\n",
    "    # ğŸ”— Format the URL with the current page number\n",
    "    url = base_url.format(page_num)\n",
    "\n",
    "    # ğŸŒ Send a GET request to the page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # ğŸœ Parse the HTML content using BeautifulSoup and lxml parser\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    # ğŸ” Select all quote blocks on the page\n",
    "    quotes = soup.select(\"div.quote\")\n",
    "\n",
    "    # ğŸ” Loop through each quote block and extract the quote and author\n",
    "    for quote in quotes:\n",
    "        text = quote.select_one(\"span.text\").text.strip()       # ğŸ“œ Extract the quote text\n",
    "        author = quote.select_one(\"small.author\").text.strip()  # âœ Extract the author's name\n",
    "        print(f\"ğŸ“ {text} â€” {author}\")  # ğŸ“¤ Print the formatted quote and author\n",
    "\n",
    "    print(\"-\" * 50)  # ğŸ“ Separator line between pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633100b7",
   "metadata": {},
   "source": [
    "### ğŸ” Explanation\n",
    "| Concept                | Description                              |\n",
    "| ---------------------- | ---------------------------------------- |\n",
    "| `{}` in URL            | Page number placeholder                  |\n",
    "| `range(1, 6)`          | Scrapes pages 1 to 5                     |\n",
    "| `.select(\"div.quote\")` | Selects each quote block                 |\n",
    "| `.select_one(...)`     | Extracts text and author from each block |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0829c7",
   "metadata": {},
   "source": [
    "## âœ… Detecting the â€œNextâ€ Button (Optional)\n",
    "\n",
    "To automate unknown number of pages, detect if a Next button exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b4fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1  # Start from the first page\n",
    "\n",
    "while True:\n",
    "    # ğŸ”— Construct the URL for the current page\n",
    "    url = f\"https://quotes.toscrape.com/page/{page}/\"\n",
    "\n",
    "    # ğŸŒ Fetch the page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # ğŸœ Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    # ğŸ” Find all quote blocks on the current page\n",
    "    quotes = soup.select(\"div.quote\")\n",
    "\n",
    "    # â›” If no quotes are found, stop the loop (end of pagination)\n",
    "    if not quotes:\n",
    "        break  # ğŸš« No more content â€” exit the loop\n",
    "\n",
    "    # âœ… Print the number of quotes found on this page\n",
    "    print(f\"ğŸ“„ Page {page} - Quotes Found: {len(quotes)}\")\n",
    "\n",
    "    # â• Move to the next page\n",
    "    page += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32485974",
   "metadata": {},
   "source": [
    "## Practice Tasks\n",
    "\n",
    "1. Scrape all quotes across all available pages using a while loop.\n",
    "\n",
    "2. Create a list of all unique authors.\n",
    "\n",
    "3. Save all quotes to a `.csv` file (we'll formally cover this in Lesson 13!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa8bba",
   "metadata": {},
   "source": [
    "## ğŸ”œ Next up: Lesson 11 â€“ Forms and Query URLs\n",
    "\n",
    "Learn how to send parameters via the URL (e.g., search filters), and scrape based on user input."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
